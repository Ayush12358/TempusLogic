{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e897f503",
   "metadata": {},
   "source": [
    "# GSM 8K tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb274c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re\n",
    "from google import genai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8b7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer question\n",
    "class GSM8K_Test:\n",
    "    # initialize with number of bad examples to include in the prompt, number of test examples to evaluate, model name, and api key\n",
    "    def __init__(self, num_bad_examples=25, num_tests=25, model_name=\"models/gemma-3-4b-it\", \n",
    "                 api_key=genai.Client(api_key=os.getenv(\"GENAI_API_KEY\")), log_filename='logs/test1.log',\n",
    "                 retry = True, llm_eval = True, test_no = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        # read only the needed number of rows from the parquet file\n",
    "        self.df = pd.read_parquet('data/gsm8k_with_bad_llm_answers.parquet')\n",
    "        \n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        # self.client = genai.Client(api_key='AIzaSyBvpujYrBWh07wqmP3gm5jgZ3ITmW1vzzo')\n",
    "        self._response_cache = {}\n",
    "        self.num_bad_examples = num_bad_examples\n",
    "        self.num_test_examples = num_tests\n",
    "        self.log_filename = log_filename\n",
    "        self.logger = None\n",
    "        self.retry = retry\n",
    "        self.llm_eval = llm_eval\n",
    "        self.test_no = test_no\n",
    "    # logging functions\n",
    "    def init_log(self):\n",
    "        log = open(self.log_filename, 'w')\n",
    "        log.write(\"Test log\\n\")\n",
    "        log.flush()\n",
    "        self.logger = log\n",
    "    def log(self, message):\n",
    "        self.logger.write(message + \"\\n\")\n",
    "        self.logger.flush()\n",
    "    def close_log(self):\n",
    "        self.logger.close()\n",
    "    # function to generate response from the model, with retries and caching\n",
    "    def generate_response(self, prompt, model_name=None, max_retries=5):\n",
    "        if self.retry:\n",
    "            return self.c_generate_response(prompt, max_retries=max_retries, model_name=model_name)\n",
    "        else:\n",
    "            return self.s_generate_response(prompt, model_name=model_name)\n",
    "    def c_generate_response(self, prompt, max_retries=5, model_name=None):\n",
    "        # Generate content with retries and simple caching.\n",
    "        # Respects RetryInfo in error messages if present (e.g. 'retryDelay': '22s').\n",
    "        # Set environment variable DRY_RUN=1 to avoid making API calls during development.\n",
    "        if model_name is not None:\n",
    "            model = model_name\n",
    "        else:\n",
    "            model = self.model_name\n",
    "        # dry-run mode for local testing\n",
    "        if os.getenv(\"DRY_RUN\") == \"1\":\n",
    "            return \"DRY_RUN_RESPONSE\"\n",
    "\n",
    "        cache_key = (model, prompt)\n",
    "        if cache_key in self._response_cache:\n",
    "            return self._response_cache[cache_key]\n",
    "\n",
    "        backoff = 1.0\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # small pause between requests to help avoid bursting the quota\n",
    "                time.sleep(0.05)\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=model,\n",
    "                    contents=prompt\n",
    "                )\n",
    "                text = getattr(response, \"text\", str(response))\n",
    "                self._response_cache[cache_key] = text\n",
    "                return text\n",
    "            except Exception as e:\n",
    "                msg = str(e)\n",
    "                # try to parse recommended retryDelay like 'retryDelay': '22s'\n",
    "                m = re.search(r\"retryDelay['\\\"]?\\s*[:=]\\s*['\\\"]?(\\d+)s\", msg)\n",
    "                if m:\n",
    "                    delay = int(m.group(1))\n",
    "                else:\n",
    "                    # fallback exponential backoff with jitter\n",
    "                    delay = backoff + (0.1 * attempt)\n",
    "\n",
    "                # if it's clearly a quota error, wait the suggested time; otherwise exponential backoff\n",
    "                if \"RESOURCE_EXHAUSTED\" in msg or \"quota\" in msg.lower() or \"RetryInfo\" in msg:\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                backoff *= 2\n",
    "                # final attempt will re-raise if it fails\n",
    "                if attempt == max_retries:\n",
    "                    raise\n",
    "    def s_generate_response(self, prompt, model_name=None):\n",
    "        if model_name is None:\n",
    "            model_name = self.model_name\n",
    "        response = self.client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt\n",
    "        )\n",
    "        text = getattr(response, \"text\", str(response))\n",
    "        return text\n",
    "    def test_generate_response(self):\n",
    "        try:\n",
    "            response = self.generate_response('What is 2 + 2?')\n",
    "        except Exception as e:\n",
    "            print(f\"Error during testing generate_response.\")\n",
    "            return\n",
    "        print (f\"Working!!\")\n",
    "    # evaluate if the generated answer is correct\n",
    "    def evaluate_answer(self, generated_answer, correct_answer):\n",
    "        if self.llm_eval:\n",
    "            return self.evaluate_answer_llm(generated_answer, correct_answer)\n",
    "        else:\n",
    "            return self.evaluate_answer_numerically(generated_answer, correct_answer)\n",
    "    def evaluate_answer_numerically(self, generated_answer, correct_answer):\n",
    "        # extract the numerical value from the correct answer\n",
    "        correct_value = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", correct_answer)\n",
    "        if len(correct_value) == 0:\n",
    "            return False\n",
    "        correct_value = correct_value[-1]  # take the last number in the answer\n",
    "        # extract the numerical value from the generated answer\n",
    "        generated_value = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", generated_answer)\n",
    "        if len(generated_value) == 0:\n",
    "            return False\n",
    "        generated_value = generated_value[-1]  # take the last number in the answer\n",
    "        return correct_value == generated_value\n",
    "    def evaluate_answer_llm(self, generated_answer, correct_answer):\n",
    "        prompt = f\"Q: {self.df.iloc[0]['question']}\\nGenerated answer is: {generated_answer}\\n\\nThe correct answer is: {correct_answer}. Check if the numerical value is the same in both the answers. Answer with a reason and a single word yes or no.\"\n",
    "        response = self.generate_response(prompt, model_name='models/gemma-3-12b-it')\n",
    "        # if it includes the word yes, return True else False\n",
    "        self.log(f\"Evaluation prompt: {prompt}\\nEvaluation response: {response}\\n\")\n",
    "        return 'yes' in response.lower()\n",
    "    # make a prompt with n bad llm answers and 1 question to be answered\n",
    "    def make_bad_prompt(self, n_bad,n_q,is_bad_prompt=True):\n",
    "        if self.test_no == 1:\n",
    "            return self.make_bad_prompt1(n_bad, n_q, is_bad_prompt)\n",
    "        elif self.test_no == 2:\n",
    "            return self.make_bad_prompt2(n_bad, n_q, is_bad_prompt)\n",
    "        else:\n",
    "            return self.make_bad_prompt3(n_bad, n_q, is_bad_prompt)\n",
    "    def make_bad_prompt1(self, n_bad,n_q,is_bad_prompt=True):\n",
    "        # m is the number of bad llm answers to include\n",
    "        # n is the question that needs to be answered\n",
    "        # return the prompt string\n",
    "        m,n = n_bad,n_q\n",
    "        prompt = \"\"\n",
    "        for i in range(m):\n",
    "            if self.df.iloc[i]['is bad answer correct'] == True:\n",
    "                continue\n",
    "            if not is_bad_prompt:\n",
    "                prompt += f\"Q: {self.df.iloc[i]['question']}\\nA: {self.df.iloc[i]['bad llm answer']}\\n\\n\"\n",
    "            else:\n",
    "                prompt += f\"Q: {self.df.iloc[i]['question']}\\nA: {self.df.iloc[i]['answer']}\\n\\n\"\n",
    "        prompt += '\\nNow correctly answer the next question.\\n'\n",
    "        prompt += f\"Q: {self.df.iloc[n]['question']}\\nA:\"\n",
    "        return prompt\n",
    "    def make_bad_prompt2(self, n_bad,n_q,is_bad_prompt=True):\n",
    "        # m is the number of bad llm answers to include\n",
    "        # n is the question that needs to be answered\n",
    "        # return the prompt string\n",
    "        m,n = n_bad,n_q\n",
    "        prompt = \"\"\n",
    "        for i in range(m):\n",
    "            if self.df.iloc[i]['is bad answer correct'] == True:\n",
    "                continue\n",
    "            if not is_bad_prompt:\n",
    "                prompt += f\"Q: {self.df.iloc[i]['question']}\\nA: {self.df.iloc[i]['bad llm answer']}\\n\\n\"\n",
    "            else:\n",
    "                prompt += f\"Q: {self.df.iloc[i]['question']}\\nA: {self.df.iloc[i]['answer']}\\n\\n\"\n",
    "        prompt += '\\nNow correctly answer the next incompletely answered question.\\n'\n",
    "        prompt += f\"Q: {self.df.iloc[n]['question']}\\nA:\"\n",
    "        answer = self.df.iloc[n]['bad llm answer']\n",
    "        # remove half the text from the bad llm answer to simulate an incomplete answer\n",
    "        half_index = len(answer) // 2\n",
    "        incomplete_answer = answer[:half_index]\n",
    "        prompt += incomplete_answer\n",
    "        return prompt\n",
    "    def make_bad_prompt3(self, n_bad,n_q,is_bad_prompt=True):\n",
    "        # m is the number of bad llm answers to include\n",
    "        # n is the question that needs to be answered\n",
    "        # return the prompt string\n",
    "        m,n = n_bad,n_q\n",
    "        prompt = \"\"\n",
    "        for i in range(m):\n",
    "            if self.df.iloc[i]['is bad answer correct'] == True:\n",
    "                continue\n",
    "            if not is_bad_prompt:\n",
    "                prompt += f\"Q: {self.df.iloc[i]['question']}\\nA: {self.df.iloc[i]['bad llm answer']}\\n\\n\"\n",
    "            else:\n",
    "                prompt += f\"Q: {self.df.iloc[i]['question']}\\nA: {self.df.iloc[i]['answer']}\\n\\n\"\n",
    "        prompt += '\\nNow correctly answer the next answered question again.\\n'\n",
    "        prompt += f\"Q: {self.df.iloc[n]['question']}\\nA:{self.df.iloc[n]['bad llm answer']}\\n\\n\"\n",
    "        return prompt\n",
    "    # test the model with bad prompts, goog prompts, and without prompts\n",
    "    def test_with_prompt(self, is_bad_prompt=True):\n",
    "        self.log(\"Starting test with bad prompt\\n\")\n",
    "        results_bad = []\n",
    "        description = \"Testing with bad prompt\" if is_bad_prompt else \"Testing with good prompt\"\n",
    "        for i in tqdm(range(self.num_bad_examples,self.num_bad_examples+self.num_test_examples), desc=description):\n",
    "            prompt = self.make_bad_prompt(self.num_bad_examples,i,is_bad_prompt)\n",
    "            try:\n",
    "                generated_answer = self.generate_response(prompt)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating response for question index {i}\")\n",
    "                results_bad.append(-1)  # indicate error with -1\n",
    "                self.log(f\"Error generating response for question index {i}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            correct_answer = self.df.iloc[i]['answer']\n",
    "            is_correct = self.evaluate_answer(generated_answer, correct_answer)\n",
    "            results_bad.append(1 if is_correct else 0)\n",
    "        return results_bad\n",
    "    def test_without_prompt(self):\n",
    "        self.log(\"Starting test without prompt\\n\")\n",
    "        results_no_prompt = []\n",
    "        for i in tqdm(range(self.num_bad_examples,self.num_bad_examples+self.num_test_examples), desc=\"Testing without prompt\"):\n",
    "            question = self.df.iloc[i]['question']\n",
    "            try:\n",
    "                generated_answer = self.generate_response(question)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating response for question index {i}: {e}\")\n",
    "                results_no_prompt.append(-1)  # indicate error with -1\n",
    "                self.log(f\"Error generating response for question index {i}\")\n",
    "                continue\n",
    "            correct_answer = self.df.iloc[i]['answer']\n",
    "            is_correct = self.evaluate_answer(generated_answer, correct_answer)\n",
    "            results_no_prompt.append(1 if is_correct else 0)\n",
    "        return results_no_prompt\n",
    "    def compare_results(self):\n",
    "        print ('\\n')\n",
    "        results_bad = self.test_with_prompt(is_bad_prompt=True)\n",
    "        results_good = self.test_with_prompt(is_bad_prompt=False)\n",
    "        results_no_prompt = self.test_without_prompt()\n",
    "        \n",
    "        correct_bad_prompt = sum(1 for r in results_bad if r == 1)\n",
    "        correct_good_prompt = sum(1 for r in results_good if r == 1)\n",
    "        correct_without_prompt = sum(1 for r in results_no_prompt if r == 1)\n",
    "        \n",
    "        inconclusive_bad_prompt = sum(1 for r in results_bad if r == -1)\n",
    "        inconclusive_good_prompt = sum(1 for r in results_good if r == -1)\n",
    "        inconclusive_without_prompt = sum(1 for r in results_no_prompt if r == -1)\n",
    "        \n",
    "        self.log(f\"Inconclusive (errors) with bad prompt: {inconclusive_bad_prompt}/{self.num_test_examples}\")\n",
    "        self.log(f\"Inconclusive (errors) with good prompt: {inconclusive_good_prompt}/{self.num_test_examples}\")\n",
    "        self.log(f\"Inconclusive (errors) without prompt: {inconclusive_without_prompt}/{self.num_test_examples}\")\n",
    "        \n",
    "        self.log(f\"Correct with bad prompt: {correct_bad_prompt}/{self.num_test_examples-inconclusive_bad_prompt}\")\n",
    "        self.log(f\"Correct with good prompt: {correct_good_prompt}/{self.num_test_examples-inconclusive_good_prompt}\")\n",
    "        self.log(f\"Correct without prompt: {correct_without_prompt}/{self.num_test_examples-inconclusive_without_prompt}\")\n",
    "        results = pd.DataFrame({\n",
    "            'results_bad_prompt': results_bad,\n",
    "            'results_good_prompt': results_good,\n",
    "            'results_without_prompt': results_no_prompt\n",
    "        })\n",
    "        return results\n",
    "class TestGSM8K:\n",
    "    def __init__(self, model_name, num_bad_examples, num_tests):\n",
    "        super().__init__()\n",
    "        self.config = {\n",
    "            'num_bad_examples': num_bad_examples,\n",
    "            'num_tests': num_tests,\n",
    "            'model_name': model_name,\n",
    "            'api_key': os.getenv(\"GENAI_API_KEY\"),\n",
    "            'retry': True,\n",
    "            'llm_eval': False\n",
    "        }\n",
    "        self.log_filename = 'logs/test.log'\n",
    "        self.logger = None\n",
    "        self.results = None\n",
    "        self.scores = None\n",
    "        self.final_score = None\n",
    "    # logging functions\n",
    "    def init_log(self):\n",
    "        log = open(self.log_filename, 'w')\n",
    "        log.write(\"Test log\\n\")\n",
    "        log.flush()\n",
    "        self.logger = log\n",
    "    def log(self, message):\n",
    "        self.logger.write(message + \"\\n\")\n",
    "        self.logger.flush()\n",
    "    def close_log(self):\n",
    "        self.logger.close()\n",
    "    # tests\n",
    "    def score(self):\n",
    "        scores = []\n",
    "        for test_name, test_results in self.results.items():\n",
    "            for (n_bad, n_test), df in test_results.items():\n",
    "                correct_bad_prompt = sum(1 for r in df['results_bad_prompt'] if r == 1)\n",
    "                correct_good_prompt = sum(1 for r in df['results_good_prompt'] if r == 1)\n",
    "                correct_without_prompt = sum(1 for r in df['results_without_prompt'] if r == 1)\n",
    "                inconclusive_bad_prompt = sum(1 for r in df['results_good_prompt'] if r == -1)\n",
    "                inconclusive_good_prompt = sum(1 for r in df['results_good_prompt'] if r == -1)\n",
    "                inconclusive_without_prompt = sum(1 for r in df['results_without_prompt'] if r == -1)\n",
    "                score_entry = {\n",
    "                    'test_name': test_name,\n",
    "                    'num_bad_examples': n_bad,\n",
    "                    'num_test_examples': n_test,\n",
    "                    'correct_bad_prompt': correct_bad_prompt,\n",
    "                    'correct_good_prompt': correct_good_prompt,\n",
    "                    'correct_without_prompt': correct_without_prompt,\n",
    "                    'inconclusive_bad_prompt': inconclusive_bad_prompt,\n",
    "                    'inconclusive_good_prompt': inconclusive_good_prompt,\n",
    "                    'inconclusive_without_prompt': inconclusive_without_prompt,\n",
    "                    'accuracy_bad_prompt': correct_bad_prompt / (n_test - inconclusive_bad_prompt) if (n_test - inconclusive_bad_prompt) > 0 else 0,\n",
    "                    'accuracy_good_prompt': correct_good_prompt / (n_test - inconclusive_good_prompt) if (n_test - inconclusive_good_prompt) > 0 else 0,\n",
    "                    'accuracy_without_prompt': correct_without_prompt / (n_test - inconclusive_without_prompt) if (n_test - inconclusive_without_prompt) > 0 else 0\n",
    "                }\n",
    "                scores.append(score_entry)\n",
    "        scores_df = pd.DataFrame(scores)\n",
    "        scores_df['accuracy_bad_deviation'] = scores_df['accuracy_bad_prompt'] - scores_df['accuracy_without_prompt']\n",
    "        scores_df['accuracy_good_deviation'] = scores_df['accuracy_good_prompt'] - scores_df['accuracy_without_prompt']\n",
    "        scores_df['accuracy_deviation'] = scores_df[['accuracy_bad_deviation', 'accuracy_good_deviation']].mean(axis=1)\n",
    "        scores_df['average_deviation'] = scores_df['accuracy_deviation'].mean()\n",
    "        self.scores = scores_df\n",
    "        deviations = scores_df['accuracy_deviation']\n",
    "        # average_deviation = deviations.mean()\n",
    "        average_std_dev = deviations.std()\n",
    "        combined_avg_score = {\n",
    "            'combined_avg_score': scores_df[['accuracy_bad_prompt', 'accuracy_good_prompt', 'accuracy_without_prompt']].mean().mean(),\n",
    "            'num_bad_examples': scores_df.iloc[0]['num_bad_examples'],\n",
    "            'num_test_examples': scores_df.iloc[0]['num_test_examples'],\n",
    "            'average_std_dev': average_std_dev\n",
    "        }\n",
    "        self.final_score = average_std_dev\n",
    "        return pd.DataFrame([combined_avg_score])\n",
    "    def test(self, test_no):\n",
    "        config = self.config\n",
    "        results = {}\n",
    "        for n_bad in config['num_bad_examples']:\n",
    "            for n_test in config['num_tests']:\n",
    "                tester = GSM8K_Test(n_bad, n_test, model_name=config['model_name'], api_key=config['api_key'],\n",
    "                                   retry=config['retry'], llm_eval=config['llm_eval'], test_no=test_no)\n",
    "                tester.init_log()\n",
    "                result = tester.compare_results()\n",
    "                tester.close_log()\n",
    "                results[(n_bad, n_test)] = result\n",
    "        return results\n",
    "    def testing(self):\n",
    "        self.init_log()\n",
    "        self.log(\"Starting GSM8K tests\\n\")\n",
    "        results = []\n",
    "        for i in range(1,4):\n",
    "            res = self.test(i)\n",
    "            results.append(res)\n",
    "            self.log(f\"Test {i} complete\\n\")\n",
    "        \n",
    "        # all tests complete\n",
    "        self.log(\"All tests complete\\n\")\n",
    "        self.close_log()\n",
    "        results = {\n",
    "            'test1': results[0],\n",
    "            'test2': results[1],\n",
    "            'test3': results[2]\n",
    "        }\n",
    "        self.results = results\n",
    "        return self.score()\n",
    "    def run_tests(self):\n",
    "        scores = self.testing()\n",
    "        scores.to_csv('results/scores.csv', mode='a', header=not os.path.exists('results/scores.csv'))\n",
    "        return self.final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de1bdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with bad prompt: 100%|██████████| 1/1 [00:07<00:00,  7.69s/it]\n",
      "Testing with good prompt: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Testing without prompt: 100%|██████████| 1/1 [00:04<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with bad prompt: 100%|██████████| 1/1 [00:08<00:00,  8.04s/it]\n",
      "Testing with good prompt: 100%|██████████| 1/1 [00:03<00:00,  3.51s/it]\n",
      "Testing without prompt: 100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with bad prompt: 100%|██████████| 1/1 [00:07<00:00,  7.87s/it]\n",
      "Testing with good prompt: 100%|██████████| 1/1 [00:03<00:00,  3.85s/it]\n",
      "Testing without prompt: 100%|██████████| 1/1 [00:04<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time taken: 49.233328104019165 seconds\n",
      "Final scores: [0.2886751345948129]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run the tests\n",
    "llms = [\"models/gemini-2.5-flash-lite\"]\n",
    "num_bad_examples = [1]\n",
    "num_tests = [1]\n",
    "scores =[]\n",
    "for llm in llms:\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "    # \n",
    "    final_score = TestGSM8K(llm, num_bad_examples, num_tests).run_tests()\n",
    "    scores.append(final_score)\n",
    "    # \n",
    "    end_time = time.time()\n",
    "    print (f\"\\nTotal time taken: {end_time - start_time} seconds\")\n",
    "print (f\"Final scores: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d72858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
